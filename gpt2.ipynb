{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b1cf009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8eb439f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self,emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "        \n",
    "    def forward(self,idx):\n",
    "        mean = idx.mean(dim=-1,keepdim = True)\n",
    "        var = idx.var(dim = -1,keepdim =True,unbiased = False)\n",
    "        norm_x = (idx - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3797d591",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadattation(nn.Module):\n",
    "    \n",
    "    def __init__(self,d_in,d_out,context_length,dropout,num_heads,qkv_bias = False):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert (d_out % num_heads == 0), \\\n",
    "        'number of heads must be lower then dimension out'\n",
    "        \n",
    "        self.register_buffer('mask',torch.triu(torch.ones(context_length,context_length),diagonal = 1))\n",
    "        self.d_out  = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = self.d_out // self.num_heads\n",
    "        self.q_weight = nn.Linear(d_in,d_out,bias = qkv_bias)\n",
    "        self.k_weight = nn.Linear(d_in,d_out,bias = qkv_bias)\n",
    "        self.v_weight = nn.Linear(d_in,d_out,bias = qkv_bias)\n",
    "        self.dropout  = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,idx):\n",
    "        batch,num_tokens,d_in = idx.shape\n",
    "        q_matrix = self.q_weight(idx)\n",
    "        k_matrix = self.k_weight(idx)\n",
    "        v_matrix = self.v_weight(idx)\n",
    "        \n",
    "        query = q_matrix.view(batch,num_tokens,self.num_heads,self.head_dim)\n",
    "        key = k_matrix.view(batch,num_tokens,self.num_heads,self.head_dim)\n",
    "        value = v_matrix.view(batch,num_tokens,self.num_heads,self.head_dim)\n",
    "        \n",
    "        query = query.transpose(1,2)\n",
    "        key = key.transpose(1,2)\n",
    "        value = value.transpose(1,2)\n",
    "        \n",
    "        attention_score = query @ key.transpose(2,3)\n",
    "        masked_matrix = attention_score.masked_fill(self.mask.bool()[:num_tokens,:num_tokens],-torch.inf)\n",
    "        attention_weight = torch.softmax(masked_matrix / idx.shape[-1] ** 0.5,dim=-1)\n",
    "        \n",
    "        attention_weight = self.dropout(attention_weight)\n",
    "        context_vec = (attention_weight @ value).transpose(1,2)\n",
    "        context_vec = context_vec.contiguous().view(batch,num_tokens,self.d_out)\n",
    "        \n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a9a9fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "880d18dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "604b8ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformerblock(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.MultiHeadattation_instance = MultiHeadattation(cfg['emb_dim'],cfg['emb_dim'],cfg['context_length'],cfg['drop_rate'],cfg['num_heads'])\n",
    "        self.att = MultiHeadattation(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"num_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNormalization(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNormalization(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x) \n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  \n",
    "        \n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "739e0fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[Transformerblock(cfg) for _ in range(cfg[\"num_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNormalization(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, idx):\n",
    "        batch_size, seq_len = idx.shape\n",
    "        tok_embeds = self.tok_emb(idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=idx.device))\n",
    "        idx = tok_embeds + pos_embeds \n",
    "        idx = self.drop_emb(idx)\n",
    "        idx = self.trf_blocks(idx)\n",
    "        idx = self.final_norm(idx)\n",
    "        logits = self.out_head(idx)\n",
    "        return logits\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e6dfbfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_Cfg = {\n",
    "    'num_heads' : 12,\n",
    "    'num_layers' : 12,\n",
    "    'drop_rate' : 0.1,\n",
    "    'qkv_bias' : False,\n",
    "    'emb_dim' : 768,\n",
    "    'context_length' : 1024,\n",
    "    'vocab_size'  : 50257,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "79e57f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "40e8d6d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2347, -0.1016,  0.1631,  ...,  0.8065,  0.3054,  0.5566],\n",
       "         [-0.3107, -0.4797,  0.4004,  ...,  0.6313, -0.7497, -0.5196],\n",
       "         [ 0.4809, -0.5083,  0.6023,  ..., -0.0438, -0.9024,  0.6370],\n",
       "         [-0.3182, -0.1915,  0.6027,  ...,  0.7526, -0.4295,  0.3570]],\n",
       "\n",
       "        [[ 0.3467, -0.1348,  0.1617,  ...,  0.5546,  0.0543,  0.0845],\n",
       "         [-0.1944, -0.2589, -0.0604,  ...,  0.4980, -0.5669,  0.8765],\n",
       "         [-0.7341, -1.0489,  0.1133,  ...,  1.3567, -0.4445,  0.3827],\n",
       "         [-0.3083, -0.2613,  0.2062,  ...,  1.4051, -0.5469,  0.2193]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummygpt = GPT2(GPT_Cfg)\n",
    "result = dummygpt(batch)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3c750529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number Of Parameters are :177,156,096\n"
     ]
    }
   ],
   "source": [
    "total_parameters = sum(p.numel() for p in dummygpt.parameters())\n",
    "print(f\"Total Number Of Parameters are :{total_parameters:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "45a048dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new_text(idx,max_iteration,model,context_size):\n",
    "    \n",
    "    for _ in range(max_iteration):\n",
    "        idx_new = idx[:,-context_size:]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_new)\n",
    "            \n",
    "        new_logits = logits[:,-1,:]\n",
    "        probas = torch.softmax(new_logits,dim=-1)\n",
    "        max_probas = torch.argmax(probas,dim=-1,keepdim=True)\n",
    "        \n",
    "        idx = torch.concat((idx,max_probas),dim=-1)\n",
    "        \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1f30c0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized text is :  [1820, 1438]\n",
      "Tokenized text shape is :  torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "text = \"my name\"\n",
    "toknized_text = tokenizer.encode(text)\n",
    "print(\"Tokenized text is : \",toknized_text)\n",
    "new_tokenized_text = torch.tensor(toknized_text).unsqueeze(0)\n",
    "print(\"Tokenized text shape is : \",new_tokenized_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "15619267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output tensor is : tensor([[ 1820,  1438,  3440, 22545, 25706, 41705, 33957,  9951]])\n"
     ]
    }
   ],
   "source": [
    "predict = generate_new_text(\n",
    "    idx = new_tokenized_text,\n",
    "    max_iteration = 6,\n",
    "    model = dummygpt,\n",
    "    context_size = GPT_Cfg['context_length']\n",
    ")\n",
    "\n",
    "print(\"output tensor is :\",predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8a5f315e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my name loadomical tan Franch Picks suspended\n"
     ]
    }
   ],
   "source": [
    "uncoded_text = tokenizer.decode(predict.squeeze(0).tolist())\n",
    "print(uncoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3f7004",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
